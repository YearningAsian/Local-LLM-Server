{
  "best_global_step": null,
  "best_metric": null,
  "best_model_checkpoint": null,
  "epoch": 0.5333333333333333,
  "eval_steps": 500,
  "global_step": 200,
  "is_hyper_param_search": false,
  "is_local_process_zero": true,
  "is_world_process_zero": true,
  "log_history": [
    {
      "epoch": 0.0026666666666666666,
      "grad_norm": NaN,
      "learning_rate": 0.0,
      "loss": 7.7759,
      "step": 1
    },
    {
      "epoch": 0.005333333333333333,
      "grad_norm": NaN,
      "learning_rate": 0.0,
      "loss": 9.0737,
      "step": 2
    },
    {
      "epoch": 0.008,
      "grad_norm": NaN,
      "learning_rate": 0.0,
      "loss": 8.526,
      "step": 3
    },
    {
      "epoch": 0.010666666666666666,
      "grad_norm": NaN,
      "learning_rate": 0.0,
      "loss": 9.0772,
      "step": 4
    },
    {
      "epoch": 0.013333333333333334,
      "grad_norm": 51.082305908203125,
      "learning_rate": 0.0,
      "loss": 8.3634,
      "step": 5
    },
    {
      "epoch": 0.016,
      "grad_norm": 47.764610290527344,
      "learning_rate": 5e-06,
      "loss": 8.5543,
      "step": 6
    },
    {
      "epoch": 0.018666666666666668,
      "grad_norm": 57.49942398071289,
      "learning_rate": 1e-05,
      "loss": 8.1137,
      "step": 7
    },
    {
      "epoch": 0.021333333333333333,
      "grad_norm": 54.74522018432617,
      "learning_rate": 1.5e-05,
      "loss": 8.4111,
      "step": 8
    },
    {
      "epoch": 0.024,
      "grad_norm": 52.5634765625,
      "learning_rate": 2e-05,
      "loss": 7.8561,
      "step": 9
    },
    {
      "epoch": 0.02666666666666667,
      "grad_norm": 52.40361404418945,
      "learning_rate": 2.5e-05,
      "loss": 7.1363,
      "step": 10
    },
    {
      "epoch": 0.029333333333333333,
      "grad_norm": 53.93169403076172,
      "learning_rate": 3e-05,
      "loss": 5.5789,
      "step": 11
    },
    {
      "epoch": 0.032,
      "grad_norm": 52.88254928588867,
      "learning_rate": 3.5e-05,
      "loss": 5.2284,
      "step": 12
    },
    {
      "epoch": 0.034666666666666665,
      "grad_norm": 51.42533493041992,
      "learning_rate": 4e-05,
      "loss": 4.8049,
      "step": 13
    },
    {
      "epoch": 0.037333333333333336,
      "grad_norm": 46.43364715576172,
      "learning_rate": 4.5e-05,
      "loss": 3.1787,
      "step": 14
    },
    {
      "epoch": 0.04,
      "grad_norm": 36.66518020629883,
      "learning_rate": 5e-05,
      "loss": 2.0955,
      "step": 15
    },
    {
      "epoch": 0.042666666666666665,
      "grad_norm": 22.331682205200195,
      "learning_rate": 4.9999900766173944e-05,
      "loss": 1.2298,
      "step": 16
    },
    {
      "epoch": 0.04533333333333334,
      "grad_norm": 7.8827643394470215,
      "learning_rate": 4.999960306548357e-05,
      "loss": 0.6459,
      "step": 17
    },
    {
      "epoch": 0.048,
      "grad_norm": 4.632687091827393,
      "learning_rate": 4.999910690029223e-05,
      "loss": 0.5271,
      "step": 18
    },
    {
      "epoch": 0.050666666666666665,
      "grad_norm": 1.1043310165405273,
      "learning_rate": 4.999841227453884e-05,
      "loss": 0.4404,
      "step": 19
    },
    {
      "epoch": 0.05333333333333334,
      "grad_norm": 0.2743087112903595,
      "learning_rate": 4.9997519193737816e-05,
      "loss": 0.4233,
      "step": 20
    },
    {
      "epoch": 0.056,
      "grad_norm": 0.1888124644756317,
      "learning_rate": 4.999642766497909e-05,
      "loss": 0.4153,
      "step": 21
    },
    {
      "epoch": 0.058666666666666666,
      "grad_norm": 0.19830209016799927,
      "learning_rate": 4.999513769692796e-05,
      "loss": 0.3996,
      "step": 22
    },
    {
      "epoch": 0.06133333333333333,
      "grad_norm": 0.20436599850654602,
      "learning_rate": 4.9993649299825116e-05,
      "loss": 0.4188,
      "step": 23
    },
    {
      "epoch": 0.064,
      "grad_norm": 0.1915062814950943,
      "learning_rate": 4.999196248548651e-05,
      "loss": 0.398,
      "step": 24
    },
    {
      "epoch": 0.06666666666666667,
      "grad_norm": 0.1970202773809433,
      "learning_rate": 4.9990077267303254e-05,
      "loss": 0.3827,
      "step": 25
    },
    {
      "epoch": 0.06933333333333333,
      "grad_norm": 0.19634966552257538,
      "learning_rate": 4.998799366024155e-05,
      "loss": 0.4101,
      "step": 26
    },
    {
      "epoch": 0.072,
      "grad_norm": 0.18907730281352997,
      "learning_rate": 4.998571168084254e-05,
      "loss": 0.4236,
      "step": 27
    },
    {
      "epoch": 0.07466666666666667,
      "grad_norm": 0.19899360835552216,
      "learning_rate": 4.998323134722218e-05,
      "loss": 0.4045,
      "step": 28
    },
    {
      "epoch": 0.07733333333333334,
      "grad_norm": 0.18648572266101837,
      "learning_rate": 4.998055267907113e-05,
      "loss": 0.406,
      "step": 29
    },
    {
      "epoch": 0.08,
      "grad_norm": 0.17389751970767975,
      "learning_rate": 4.997767569765452e-05,
      "loss": 0.3888,
      "step": 30
    },
    {
      "epoch": 0.08266666666666667,
      "grad_norm": 0.1840987503528595,
      "learning_rate": 4.997460042581188e-05,
      "loss": 0.3898,
      "step": 31
    },
    {
      "epoch": 0.08533333333333333,
      "grad_norm": 0.18158365786075592,
      "learning_rate": 4.997132688795689e-05,
      "loss": 0.3983,
      "step": 32
    },
    {
      "epoch": 0.088,
      "grad_norm": 0.1771593689918518,
      "learning_rate": 4.9967855110077187e-05,
      "loss": 0.3843,
      "step": 33
    },
    {
      "epoch": 0.09066666666666667,
      "grad_norm": 0.17052587866783142,
      "learning_rate": 4.996418511973422e-05,
      "loss": 0.3848,
      "step": 34
    },
    {
      "epoch": 0.09333333333333334,
      "grad_norm": 0.16304469108581543,
      "learning_rate": 4.9960316946062944e-05,
      "loss": 0.3737,
      "step": 35
    },
    {
      "epoch": 0.096,
      "grad_norm": 0.16992373764514923,
      "learning_rate": 4.995625061977167e-05,
      "loss": 0.3902,
      "step": 36
    },
    {
      "epoch": 0.09866666666666667,
      "grad_norm": 0.16202996671199799,
      "learning_rate": 4.995198617314175e-05,
      "loss": 0.3796,
      "step": 37
    },
    {
      "epoch": 0.10133333333333333,
      "grad_norm": 0.16898764669895172,
      "learning_rate": 4.994752364002737e-05,
      "loss": 0.3891,
      "step": 38
    },
    {
      "epoch": 0.104,
      "grad_norm": 0.1742773801088333,
      "learning_rate": 4.9942863055855295e-05,
      "loss": 0.3897,
      "step": 39
    },
    {
      "epoch": 0.10666666666666667,
      "grad_norm": 0.16546204686164856,
      "learning_rate": 4.993800445762451e-05,
      "loss": 0.3829,
      "step": 40
    },
    {
      "epoch": 0.10933333333333334,
      "grad_norm": 0.1681963950395584,
      "learning_rate": 4.9932947883906e-05,
      "loss": 0.3859,
      "step": 41
    },
    {
      "epoch": 0.112,
      "grad_norm": 0.17044106125831604,
      "learning_rate": 4.992769337484243e-05,
      "loss": 0.36,
      "step": 42
    },
    {
      "epoch": 0.11466666666666667,
      "grad_norm": 0.1660912036895752,
      "learning_rate": 4.9922240972147795e-05,
      "loss": 0.3828,
      "step": 43
    },
    {
      "epoch": 0.11733333333333333,
      "grad_norm": 0.15614306926727295,
      "learning_rate": 4.9916590719107115e-05,
      "loss": 0.3686,
      "step": 44
    },
    {
      "epoch": 0.12,
      "grad_norm": 0.16693836450576782,
      "learning_rate": 4.991074266057609e-05,
      "loss": 0.3741,
      "step": 45
    },
    {
      "epoch": 0.12266666666666666,
      "grad_norm": 0.16783669590950012,
      "learning_rate": 4.9904696842980734e-05,
      "loss": 0.3656,
      "step": 46
    },
    {
      "epoch": 0.12533333333333332,
      "grad_norm": 0.1657777577638626,
      "learning_rate": 4.989845331431703e-05,
      "loss": 0.3736,
      "step": 47
    },
    {
      "epoch": 0.128,
      "grad_norm": 0.16707350313663483,
      "learning_rate": 4.989201212415051e-05,
      "loss": 0.363,
      "step": 48
    },
    {
      "epoch": 0.13066666666666665,
      "grad_norm": 0.16765044629573822,
      "learning_rate": 4.988537332361588e-05,
      "loss": 0.3629,
      "step": 49
    },
    {
      "epoch": 0.13333333333333333,
      "grad_norm": 0.16799573600292206,
      "learning_rate": 4.9878536965416636e-05,
      "loss": 0.3447,
      "step": 50
    },
    {
      "epoch": 0.136,
      "grad_norm": 0.17207080125808716,
      "learning_rate": 4.987150310382461e-05,
      "loss": 0.3372,
      "step": 51
    },
    {
      "epoch": 0.13866666666666666,
      "grad_norm": 0.1607048511505127,
      "learning_rate": 4.9864271794679566e-05,
      "loss": 0.3314,
      "step": 52
    },
    {
      "epoch": 0.14133333333333334,
      "grad_norm": 0.15438273549079895,
      "learning_rate": 4.985684309538875e-05,
      "loss": 0.3167,
      "step": 53
    },
    {
      "epoch": 0.144,
      "grad_norm": 0.1676623523235321,
      "learning_rate": 4.984921706492641e-05,
      "loss": 0.3237,
      "step": 54
    },
    {
      "epoch": 0.14666666666666667,
      "grad_norm": 0.1701117753982544,
      "learning_rate": 4.984139376383336e-05,
      "loss": 0.3332,
      "step": 55
    },
    {
      "epoch": 0.14933333333333335,
      "grad_norm": 0.172136127948761,
      "learning_rate": 4.98333732542165e-05,
      "loss": 0.3284,
      "step": 56
    },
    {
      "epoch": 0.152,
      "grad_norm": 0.16680166125297546,
      "learning_rate": 4.982515559974829e-05,
      "loss": 0.2997,
      "step": 57
    },
    {
      "epoch": 0.15466666666666667,
      "grad_norm": 0.17760148644447327,
      "learning_rate": 4.9816740865666266e-05,
      "loss": 0.3142,
      "step": 58
    },
    {
      "epoch": 0.15733333333333333,
      "grad_norm": 0.18037471175193787,
      "learning_rate": 4.980812911877254e-05,
      "loss": 0.3096,
      "step": 59
    },
    {
      "epoch": 0.16,
      "grad_norm": 0.16574032604694366,
      "learning_rate": 4.979932042743324e-05,
      "loss": 0.3005,
      "step": 60
    },
    {
      "epoch": 0.16266666666666665,
      "grad_norm": 0.1795729696750641,
      "learning_rate": 4.979031486157797e-05,
      "loss": 0.3087,
      "step": 61
    },
    {
      "epoch": 0.16533333333333333,
      "grad_norm": 0.17407357692718506,
      "learning_rate": 4.9781112492699275e-05,
      "loss": 0.2809,
      "step": 62
    },
    {
      "epoch": 0.168,
      "grad_norm": 0.18515099585056305,
      "learning_rate": 4.977171339385206e-05,
      "loss": 0.3042,
      "step": 63
    },
    {
      "epoch": 0.17066666666666666,
      "grad_norm": 0.1834423542022705,
      "learning_rate": 4.9762117639653006e-05,
      "loss": 0.2984,
      "step": 64
    },
    {
      "epoch": 0.17333333333333334,
      "grad_norm": 0.18350917100906372,
      "learning_rate": 4.975232530627998e-05,
      "loss": 0.2728,
      "step": 65
    },
    {
      "epoch": 0.176,
      "grad_norm": 0.19506490230560303,
      "learning_rate": 4.974233647147144e-05,
      "loss": 0.2783,
      "step": 66
    },
    {
      "epoch": 0.17866666666666667,
      "grad_norm": 0.19358588755130768,
      "learning_rate": 4.9732151214525816e-05,
      "loss": 0.2699,
      "step": 67
    },
    {
      "epoch": 0.18133333333333335,
      "grad_norm": 0.19068734347820282,
      "learning_rate": 4.9721769616300865e-05,
      "loss": 0.2605,
      "step": 68
    },
    {
      "epoch": 0.184,
      "grad_norm": 0.2008601278066635,
      "learning_rate": 4.971119175921305e-05,
      "loss": 0.2709,
      "step": 69
    },
    {
      "epoch": 0.18666666666666668,
      "grad_norm": 0.20324979722499847,
      "learning_rate": 4.970041772723685e-05,
      "loss": 0.2623,
      "step": 70
    },
    {
      "epoch": 0.18933333333333333,
      "grad_norm": 0.20107844471931458,
      "learning_rate": 4.968944760590416e-05,
      "loss": 0.2566,
      "step": 71
    },
    {
      "epoch": 0.192,
      "grad_norm": 0.2080293893814087,
      "learning_rate": 4.967828148230354e-05,
      "loss": 0.2364,
      "step": 72
    },
    {
      "epoch": 0.19466666666666665,
      "grad_norm": 0.21074460446834564,
      "learning_rate": 4.966691944507956e-05,
      "loss": 0.2306,
      "step": 73
    },
    {
      "epoch": 0.19733333333333333,
      "grad_norm": 0.2277536690235138,
      "learning_rate": 4.9655361584432106e-05,
      "loss": 0.2429,
      "step": 74
    },
    {
      "epoch": 0.2,
      "grad_norm": 0.2176368683576584,
      "learning_rate": 4.964360799211562e-05,
      "loss": 0.2195,
      "step": 75
    },
    {
      "epoch": 0.20266666666666666,
      "grad_norm": 0.2541230320930481,
      "learning_rate": 4.963165876143844e-05,
      "loss": 0.2359,
      "step": 76
    },
    {
      "epoch": 0.20533333333333334,
      "grad_norm": 0.23314841091632843,
      "learning_rate": 4.9619513987261966e-05,
      "loss": 0.2038,
      "step": 77
    },
    {
      "epoch": 0.208,
      "grad_norm": 0.24312271177768707,
      "learning_rate": 4.960717376600002e-05,
      "loss": 0.2186,
      "step": 78
    },
    {
      "epoch": 0.21066666666666667,
      "grad_norm": 0.24537144601345062,
      "learning_rate": 4.959463819561797e-05,
      "loss": 0.2025,
      "step": 79
    },
    {
      "epoch": 0.21333333333333335,
      "grad_norm": 0.23766019940376282,
      "learning_rate": 4.9581907375632034e-05,
      "loss": 0.204,
      "step": 80
    },
    {
      "epoch": 0.216,
      "grad_norm": 0.23393729329109192,
      "learning_rate": 4.956898140710845e-05,
      "loss": 0.1824,
      "step": 81
    },
    {
      "epoch": 0.21866666666666668,
      "grad_norm": 0.23728148639202118,
      "learning_rate": 4.9555860392662683e-05,
      "loss": 0.1861,
      "step": 82
    },
    {
      "epoch": 0.22133333333333333,
      "grad_norm": 0.24238763749599457,
      "learning_rate": 4.954254443645861e-05,
      "loss": 0.1791,
      "step": 83
    },
    {
      "epoch": 0.224,
      "grad_norm": 0.229088693857193,
      "learning_rate": 4.952903364420769e-05,
      "loss": 0.1643,
      "step": 84
    },
    {
      "epoch": 0.22666666666666666,
      "grad_norm": 0.2592610716819763,
      "learning_rate": 4.951532812316814e-05,
      "loss": 0.1666,
      "step": 85
    },
    {
      "epoch": 0.22933333333333333,
      "grad_norm": 0.2156154066324234,
      "learning_rate": 4.9501427982144054e-05,
      "loss": 0.1573,
      "step": 86
    },
    {
      "epoch": 0.232,
      "grad_norm": 0.2765517234802246,
      "learning_rate": 4.948733333148456e-05,
      "loss": 0.1838,
      "step": 87
    },
    {
      "epoch": 0.23466666666666666,
      "grad_norm": 0.24803707003593445,
      "learning_rate": 4.947304428308298e-05,
      "loss": 0.1585,
      "step": 88
    },
    {
      "epoch": 0.23733333333333334,
      "grad_norm": 0.23669448494911194,
      "learning_rate": 4.945856095037583e-05,
      "loss": 0.1645,
      "step": 89
    },
    {
      "epoch": 0.24,
      "grad_norm": 0.29903361201286316,
      "learning_rate": 4.944388344834205e-05,
      "loss": 0.1596,
      "step": 90
    },
    {
      "epoch": 0.24266666666666667,
      "grad_norm": 0.2747502624988556,
      "learning_rate": 4.9429011893502016e-05,
      "loss": 0.1527,
      "step": 91
    },
    {
      "epoch": 0.24533333333333332,
      "grad_norm": 0.24979954957962036,
      "learning_rate": 4.941394640391662e-05,
      "loss": 0.1421,
      "step": 92
    },
    {
      "epoch": 0.248,
      "grad_norm": 0.25391021370887756,
      "learning_rate": 4.939868709918637e-05,
      "loss": 0.1473,
      "step": 93
    },
    {
      "epoch": 0.25066666666666665,
      "grad_norm": 0.25062036514282227,
      "learning_rate": 4.9383234100450394e-05,
      "loss": 0.1447,
      "step": 94
    },
    {
      "epoch": 0.25333333333333335,
      "grad_norm": 0.24118223786354065,
      "learning_rate": 4.9367587530385505e-05,
      "loss": 0.1379,
      "step": 95
    },
    {
      "epoch": 0.256,
      "grad_norm": 0.2311708927154541,
      "learning_rate": 4.9351747513205234e-05,
      "loss": 0.1399,
      "step": 96
    },
    {
      "epoch": 0.25866666666666666,
      "grad_norm": 0.24835579097270966,
      "learning_rate": 4.933571417465881e-05,
      "loss": 0.1456,
      "step": 97
    },
    {
      "epoch": 0.2613333333333333,
      "grad_norm": 0.2297261357307434,
      "learning_rate": 4.931948764203019e-05,
      "loss": 0.1165,
      "step": 98
    },
    {
      "epoch": 0.264,
      "grad_norm": 0.25022855401039124,
      "learning_rate": 4.930306804413707e-05,
      "loss": 0.1015,
      "step": 99
    },
    {
      "epoch": 0.26666666666666666,
      "grad_norm": 0.23194670677185059,
      "learning_rate": 4.9286455511329796e-05,
      "loss": 0.1115,
      "step": 100
    },
    {
      "epoch": 0.2693333333333333,
      "grad_norm": 0.2450968325138092,
      "learning_rate": 4.926965017549039e-05,
      "loss": 0.1077,
      "step": 101
    },
    {
      "epoch": 0.272,
      "grad_norm": 0.2241780310869217,
      "learning_rate": 4.925265217003146e-05,
      "loss": 0.1229,
      "step": 102
    },
    {
      "epoch": 0.27466666666666667,
      "grad_norm": 0.23591837286949158,
      "learning_rate": 4.923546162989519e-05,
      "loss": 0.1133,
      "step": 103
    },
    {
      "epoch": 0.2773333333333333,
      "grad_norm": 0.2429077923297882,
      "learning_rate": 4.921807869155222e-05,
      "loss": 0.099,
      "step": 104
    },
    {
      "epoch": 0.28,
      "grad_norm": 0.2354428470134735,
      "learning_rate": 4.920050349300059e-05,
      "loss": 0.0989,
      "step": 105
    },
    {
      "epoch": 0.2826666666666667,
      "grad_norm": 0.26057761907577515,
      "learning_rate": 4.918273617376463e-05,
      "loss": 0.1007,
      "step": 106
    },
    {
      "epoch": 0.2853333333333333,
      "grad_norm": 0.21309852600097656,
      "learning_rate": 4.916477687489388e-05,
      "loss": 0.0872,
      "step": 107
    },
    {
      "epoch": 0.288,
      "grad_norm": 0.32085075974464417,
      "learning_rate": 4.914662573896192e-05,
      "loss": 0.0921,
      "step": 108
    },
    {
      "epoch": 0.2906666666666667,
      "grad_norm": 0.2511381506919861,
      "learning_rate": 4.9128282910065296e-05,
      "loss": 0.0945,
      "step": 109
    },
    {
      "epoch": 0.29333333333333333,
      "grad_norm": 0.2372118979692459,
      "learning_rate": 4.9109748533822315e-05,
      "loss": 0.0896,
      "step": 110
    },
    {
      "epoch": 0.296,
      "grad_norm": 0.21339581906795502,
      "learning_rate": 4.9091022757371964e-05,
      "loss": 0.0962,
      "step": 111
    },
    {
      "epoch": 0.2986666666666667,
      "grad_norm": 0.21951653063297272,
      "learning_rate": 4.9072105729372665e-05,
      "loss": 0.0929,
      "step": 112
    },
    {
      "epoch": 0.30133333333333334,
      "grad_norm": 0.20358431339263916,
      "learning_rate": 4.9052997600001146e-05,
      "loss": 0.0836,
      "step": 113
    },
    {
      "epoch": 0.304,
      "grad_norm": 0.24200664460659027,
      "learning_rate": 4.903369852095124e-05,
      "loss": 0.0938,
      "step": 114
    },
    {
      "epoch": 0.30666666666666664,
      "grad_norm": 0.21307526528835297,
      "learning_rate": 4.901420864543265e-05,
      "loss": 0.096,
      "step": 115
    },
    {
      "epoch": 0.30933333333333335,
      "grad_norm": 0.20178711414337158,
      "learning_rate": 4.899452812816978e-05,
      "loss": 0.087,
      "step": 116
    },
    {
      "epoch": 0.312,
      "grad_norm": 0.22713492810726166,
      "learning_rate": 4.897465712540046e-05,
      "loss": 0.0984,
      "step": 117
    },
    {
      "epoch": 0.31466666666666665,
      "grad_norm": 0.2145737260580063,
      "learning_rate": 4.895459579487476e-05,
      "loss": 0.1058,
      "step": 118
    },
    {
      "epoch": 0.31733333333333336,
      "grad_norm": 0.22597768902778625,
      "learning_rate": 4.8934344295853664e-05,
      "loss": 0.0849,
      "step": 119
    },
    {
      "epoch": 0.32,
      "grad_norm": 0.21758107841014862,
      "learning_rate": 4.891390278910788e-05,
      "loss": 0.0872,
      "step": 120
    },
    {
      "epoch": 0.32266666666666666,
      "grad_norm": 0.2380582094192505,
      "learning_rate": 4.8893271436916524e-05,
      "loss": 0.0879,
      "step": 121
    },
    {
      "epoch": 0.3253333333333333,
      "grad_norm": 0.23707234859466553,
      "learning_rate": 4.887245040306584e-05,
      "loss": 0.0756,
      "step": 122
    },
    {
      "epoch": 0.328,
      "grad_norm": 0.21649596095085144,
      "learning_rate": 4.885143985284789e-05,
      "loss": 0.0658,
      "step": 123
    },
    {
      "epoch": 0.33066666666666666,
      "grad_norm": 0.23062776029109955,
      "learning_rate": 4.883023995305925e-05,
      "loss": 0.0669,
      "step": 124
    },
    {
      "epoch": 0.3333333333333333,
      "grad_norm": 0.2453206330537796,
      "learning_rate": 4.8808850871999714e-05,
      "loss": 0.0778,
      "step": 125
    },
    {
      "epoch": 0.336,
      "grad_norm": 0.239455446600914,
      "learning_rate": 4.878727277947089e-05,
      "loss": 0.0868,
      "step": 126
    },
    {
      "epoch": 0.33866666666666667,
      "grad_norm": 0.21584179997444153,
      "learning_rate": 4.876550584677493e-05,
      "loss": 0.0668,
      "step": 127
    },
    {
      "epoch": 0.3413333333333333,
      "grad_norm": 0.22171495854854584,
      "learning_rate": 4.87435502467131e-05,
      "loss": 0.0759,
      "step": 128
    },
    {
      "epoch": 0.344,
      "grad_norm": 0.2667982876300812,
      "learning_rate": 4.872140615358446e-05,
      "loss": 0.0653,
      "step": 129
    },
    {
      "epoch": 0.3466666666666667,
      "grad_norm": 0.2396652102470398,
      "learning_rate": 4.8699073743184454e-05,
      "loss": 0.073,
      "step": 130
    },
    {
      "epoch": 0.34933333333333333,
      "grad_norm": 0.2673489451408386,
      "learning_rate": 4.8676553192803536e-05,
      "loss": 0.0726,
      "step": 131
    },
    {
      "epoch": 0.352,
      "grad_norm": 0.24987828731536865,
      "learning_rate": 4.865384468122573e-05,
      "loss": 0.0775,
      "step": 132
    },
    {
      "epoch": 0.3546666666666667,
      "grad_norm": 0.2450331449508667,
      "learning_rate": 4.8630948388727225e-05,
      "loss": 0.0633,
      "step": 133
    },
    {
      "epoch": 0.35733333333333334,
      "grad_norm": 0.27698665857315063,
      "learning_rate": 4.860786449707498e-05,
      "loss": 0.0801,
      "step": 134
    },
    {
      "epoch": 0.36,
      "grad_norm": 0.2513200044631958,
      "learning_rate": 4.8584593189525204e-05,
      "loss": 0.0732,
      "step": 135
    },
    {
      "epoch": 0.3626666666666667,
      "grad_norm": 0.2662145793437958,
      "learning_rate": 4.8561134650821985e-05,
      "loss": 0.0689,
      "step": 136
    },
    {
      "epoch": 0.36533333333333334,
      "grad_norm": 0.19767582416534424,
      "learning_rate": 4.8537489067195744e-05,
      "loss": 0.0611,
      "step": 137
    },
    {
      "epoch": 0.368,
      "grad_norm": 0.2512779235839844,
      "learning_rate": 4.851365662636184e-05,
      "loss": 0.0645,
      "step": 138
    },
    {
      "epoch": 0.37066666666666664,
      "grad_norm": 0.24756184220314026,
      "learning_rate": 4.8489637517519016e-05,
      "loss": 0.0696,
      "step": 139
    },
    {
      "epoch": 0.37333333333333335,
      "grad_norm": 0.2366192489862442,
      "learning_rate": 4.8465431931347904e-05,
      "loss": 0.0706,
      "step": 140
    },
    {
      "epoch": 0.376,
      "grad_norm": 0.23606762290000916,
      "learning_rate": 4.844104006000955e-05,
      "loss": 0.0756,
      "step": 141
    },
    {
      "epoch": 0.37866666666666665,
      "grad_norm": 0.22391457855701447,
      "learning_rate": 4.8416462097143846e-05,
      "loss": 0.0598,
      "step": 142
    },
    {
      "epoch": 0.38133333333333336,
      "grad_norm": 0.23003508150577545,
      "learning_rate": 4.839169823786802e-05,
      "loss": 0.0662,
      "step": 143
    },
    {
      "epoch": 0.384,
      "grad_norm": 0.22228562831878662,
      "learning_rate": 4.836674867877507e-05,
      "loss": 0.0719,
      "step": 144
    },
    {
      "epoch": 0.38666666666666666,
      "grad_norm": 0.2255392223596573,
      "learning_rate": 4.83416136179322e-05,
      "loss": 0.074,
      "step": 145
    },
    {
      "epoch": 0.3893333333333333,
      "grad_norm": 0.23868915438652039,
      "learning_rate": 4.831629325487929e-05,
      "loss": 0.0734,
      "step": 146
    },
    {
      "epoch": 0.392,
      "grad_norm": 0.22410191595554352,
      "learning_rate": 4.8290787790627254e-05,
      "loss": 0.0601,
      "step": 147
    },
    {
      "epoch": 0.39466666666666667,
      "grad_norm": 0.2123771607875824,
      "learning_rate": 4.826509742765647e-05,
      "loss": 0.0644,
      "step": 148
    },
    {
      "epoch": 0.3973333333333333,
      "grad_norm": 0.2229478806257248,
      "learning_rate": 4.823922236991518e-05,
      "loss": 0.0532,
      "step": 149
    },
    {
      "epoch": 0.4,
      "grad_norm": 0.24034623801708221,
      "learning_rate": 4.821316282281788e-05,
      "loss": 0.0563,
      "step": 150
    },
    {
      "epoch": 0.4026666666666667,
      "grad_norm": 0.230122908949852,
      "learning_rate": 4.818691899324362e-05,
      "loss": 0.0651,
      "step": 151
    },
    {
      "epoch": 0.4053333333333333,
      "grad_norm": 0.19004501402378082,
      "learning_rate": 4.816049108953449e-05,
      "loss": 0.0591,
      "step": 152
    },
    {
      "epoch": 0.408,
      "grad_norm": 0.21581116318702698,
      "learning_rate": 4.813387932149382e-05,
      "loss": 0.0494,
      "step": 153
    },
    {
      "epoch": 0.4106666666666667,
      "grad_norm": 0.22199304401874542,
      "learning_rate": 4.8107083900384624e-05,
      "loss": 0.0639,
      "step": 154
    },
    {
      "epoch": 0.41333333333333333,
      "grad_norm": 0.23923905193805695,
      "learning_rate": 4.808010503892788e-05,
      "loss": 0.0587,
      "step": 155
    },
    {
      "epoch": 0.416,
      "grad_norm": 0.21436145901679993,
      "learning_rate": 4.805294295130083e-05,
      "loss": 0.0555,
      "step": 156
    },
    {
      "epoch": 0.4186666666666667,
      "grad_norm": 0.2650292217731476,
      "learning_rate": 4.802559785313532e-05,
      "loss": 0.0605,
      "step": 157
    },
    {
      "epoch": 0.42133333333333334,
      "grad_norm": 0.21557539701461792,
      "learning_rate": 4.799806996151602e-05,
      "loss": 0.0496,
      "step": 158
    },
    {
      "epoch": 0.424,
      "grad_norm": 0.23425772786140442,
      "learning_rate": 4.79703594949788e-05,
      "loss": 0.0524,
      "step": 159
    },
    {
      "epoch": 0.4266666666666667,
      "grad_norm": 0.23283545672893524,
      "learning_rate": 4.794246667350889e-05,
      "loss": 0.0489,
      "step": 160
    },
    {
      "epoch": 0.42933333333333334,
      "grad_norm": 0.2384958416223526,
      "learning_rate": 4.791439171853921e-05,
      "loss": 0.0594,
      "step": 161
    },
    {
      "epoch": 0.432,
      "grad_norm": 0.2275964915752411,
      "learning_rate": 4.788613485294858e-05,
      "loss": 0.0526,
      "step": 162
    },
    {
      "epoch": 0.43466666666666665,
      "grad_norm": 0.2450057417154312,
      "learning_rate": 4.785769630105994e-05,
      "loss": 0.0535,
      "step": 163
    },
    {
      "epoch": 0.43733333333333335,
      "grad_norm": 0.2252182513475418,
      "learning_rate": 4.78290762886386e-05,
      "loss": 0.0592,
      "step": 164
    },
    {
      "epoch": 0.44,
      "grad_norm": 0.21617573499679565,
      "learning_rate": 4.780027504289042e-05,
      "loss": 0.0468,
      "step": 165
    },
    {
      "epoch": 0.44266666666666665,
      "grad_norm": 0.21820512413978577,
      "learning_rate": 4.7771292792460044e-05,
      "loss": 0.0385,
      "step": 166
    },
    {
      "epoch": 0.44533333333333336,
      "grad_norm": 0.23672723770141602,
      "learning_rate": 4.7742129767429026e-05,
      "loss": 0.0582,
      "step": 167
    },
    {
      "epoch": 0.448,
      "grad_norm": 0.2467384934425354,
      "learning_rate": 4.771278619931404e-05,
      "loss": 0.053,
      "step": 168
    },
    {
      "epoch": 0.45066666666666666,
      "grad_norm": 0.2515135109424591,
      "learning_rate": 4.768326232106506e-05,
      "loss": 0.0562,
      "step": 169
    },
    {
      "epoch": 0.4533333333333333,
      "grad_norm": 0.25742095708847046,
      "learning_rate": 4.765355836706349e-05,
      "loss": 0.0519,
      "step": 170
    },
    {
      "epoch": 0.456,
      "grad_norm": 0.22365066409111023,
      "learning_rate": 4.762367457312027e-05,
      "loss": 0.0427,
      "step": 171
    },
    {
      "epoch": 0.45866666666666667,
      "grad_norm": 0.2370399683713913,
      "learning_rate": 4.759361117647407e-05,
      "loss": 0.0524,
      "step": 172
    },
    {
      "epoch": 0.4613333333333333,
      "grad_norm": 0.18359427154064178,
      "learning_rate": 4.7563368415789356e-05,
      "loss": 0.0454,
      "step": 173
    },
    {
      "epoch": 0.464,
      "grad_norm": 0.22479784488677979,
      "learning_rate": 4.753294653115451e-05,
      "loss": 0.0534,
      "step": 174
    },
    {
      "epoch": 0.4666666666666667,
      "grad_norm": 0.2524249255657196,
      "learning_rate": 4.750234576407994e-05,
      "loss": 0.0436,
      "step": 175
    },
    {
      "epoch": 0.4693333333333333,
      "grad_norm": 0.2460915893316269,
      "learning_rate": 4.747156635749613e-05,
      "loss": 0.0416,
      "step": 176
    },
    {
      "epoch": 0.472,
      "grad_norm": 0.2412615716457367,
      "learning_rate": 4.744060855575177e-05,
      "loss": 0.0485,
      "step": 177
    },
    {
      "epoch": 0.4746666666666667,
      "grad_norm": 0.27175748348236084,
      "learning_rate": 4.740947260461172e-05,
      "loss": 0.046,
      "step": 178
    },
    {
      "epoch": 0.47733333333333333,
      "grad_norm": 0.23418863117694855,
      "learning_rate": 4.737815875125516e-05,
      "loss": 0.052,
      "step": 179
    },
    {
      "epoch": 0.48,
      "grad_norm": 0.23260727524757385,
      "learning_rate": 4.734666724427356e-05,
      "loss": 0.0484,
      "step": 180
    },
    {
      "epoch": 0.4826666666666667,
      "grad_norm": 0.256021648645401,
      "learning_rate": 4.7314998333668746e-05,
      "loss": 0.0449,
      "step": 181
    },
    {
      "epoch": 0.48533333333333334,
      "grad_norm": 0.22759011387825012,
      "learning_rate": 4.728315227085089e-05,
      "loss": 0.0437,
      "step": 182
    },
    {
      "epoch": 0.488,
      "grad_norm": 0.2501298487186432,
      "learning_rate": 4.725112930863652e-05,
      "loss": 0.047,
      "step": 183
    },
    {
      "epoch": 0.49066666666666664,
      "grad_norm": 0.23416632413864136,
      "learning_rate": 4.721892970124652e-05,
      "loss": 0.0434,
      "step": 184
    },
    {
      "epoch": 0.49333333333333335,
      "grad_norm": 0.25857898592948914,
      "learning_rate": 4.718655370430411e-05,
      "loss": 0.0377,
      "step": 185
    },
    {
      "epoch": 0.496,
      "grad_norm": 0.2791350781917572,
      "learning_rate": 4.715400157483282e-05,
      "loss": 0.0453,
      "step": 186
    },
    {
      "epoch": 0.49866666666666665,
      "grad_norm": 0.23119515180587769,
      "learning_rate": 4.7121273571254434e-05,
      "loss": 0.0367,
      "step": 187
    },
    {
      "epoch": 0.5013333333333333,
      "grad_norm": 0.18586644530296326,
      "learning_rate": 4.7088369953386956e-05,
      "loss": 0.0352,
      "step": 188
    },
    {
      "epoch": 0.504,
      "grad_norm": 0.23152723908424377,
      "learning_rate": 4.705529098244253e-05,
      "loss": 0.0372,
      "step": 189
    },
    {
      "epoch": 0.5066666666666667,
      "grad_norm": 0.23910872638225555,
      "learning_rate": 4.702203692102539e-05,
      "loss": 0.0452,
      "step": 190
    },
    {
      "epoch": 0.5093333333333333,
      "grad_norm": 0.24413958191871643,
      "learning_rate": 4.698860803312976e-05,
      "loss": 0.0442,
      "step": 191
    },
    {
      "epoch": 0.512,
      "grad_norm": 0.22117452323436737,
      "learning_rate": 4.6955004584137744e-05,
      "loss": 0.038,
      "step": 192
    },
    {
      "epoch": 0.5146666666666667,
      "grad_norm": 0.24173015356063843,
      "learning_rate": 4.692122684081725e-05,
      "loss": 0.0463,
      "step": 193
    },
    {
      "epoch": 0.5173333333333333,
      "grad_norm": 0.1797901839017868,
      "learning_rate": 4.688727507131986e-05,
      "loss": 0.0358,
      "step": 194
    },
    {
      "epoch": 0.52,
      "grad_norm": 0.22265096008777618,
      "learning_rate": 4.68531495451787e-05,
      "loss": 0.0399,
      "step": 195
    },
    {
      "epoch": 0.5226666666666666,
      "grad_norm": 0.18112978339195251,
      "learning_rate": 4.681885053330627e-05,
      "loss": 0.0361,
      "step": 196
    },
    {
      "epoch": 0.5253333333333333,
      "grad_norm": 0.2067209631204605,
      "learning_rate": 4.678437830799236e-05,
      "loss": 0.0402,
      "step": 197
    },
    {
      "epoch": 0.528,
      "grad_norm": 0.21734285354614258,
      "learning_rate": 4.674973314290184e-05,
      "loss": 0.0374,
      "step": 198
    },
    {
      "epoch": 0.5306666666666666,
      "grad_norm": 0.22193969786167145,
      "learning_rate": 4.6714915313072474e-05,
      "loss": 0.0362,
      "step": 199
    },
    {
      "epoch": 0.5333333333333333,
      "grad_norm": 0.20888760685920715,
      "learning_rate": 4.6679925094912795e-05,
      "loss": 0.0367,
      "step": 200
    }
  ],
  "logging_steps": 1,
  "max_steps": 1125,
  "num_input_tokens_seen": 0,
  "num_train_epochs": 3,
  "save_steps": 100,
  "stateful_callbacks": {
    "TrainerControl": {
      "args": {
        "should_epoch_stop": false,
        "should_evaluate": false,
        "should_log": false,
        "should_save": true,
        "should_training_stop": false
      },
      "attributes": {}
    }
  },
  "total_flos": 1.476868259708928e+17,
  "train_batch_size": 1,
  "trial_name": null,
  "trial_params": null
}
